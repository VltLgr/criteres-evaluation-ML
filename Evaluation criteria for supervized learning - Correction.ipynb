{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critères d'évaluation en apprentissage supervisé\n",
    "\n",
    "## Théorie\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Tout au long de ce cours nous avons étudié différentes méthodes d'apprentissage supervisé, et à chaque fois nous avons cherché à optimiser l'_accuracy_, c'est-à-dire à minimiser l'erreur de généralisation. Mais cette approche pour évaluer la pertinence de notre modèle et sa robustesse est-elle suffisante ? En particulier dans ce notebook nous allons nous intéresser au cas de la classification binaire (deux classes), nous allons voir que dans le cas de la classification, plusieurs métriques interviennent, elles peuvent être contradictoires (dans le sens où elles varient de manière opposée) et il sera donc nécessaire de faire un compromis sur certaines grandeurs en fonction de l'application que l'on considère. \n",
    "\n",
    "Considérons un exemple de classification binaire que nous déroulerons tout au long de cette partie théorique pour illustrer nos propos. Nous allons considérer une tâche de classification simple consistant à identifier des chiens dans des images, notre système prend donc en entrée des images et prédit en sortie la présence ou l'absence de chien dans cette image.\n",
    "\n",
    "### Matrice de confusion\n",
    "\n",
    "Une métrique très souvent utilisée en classification est la matrice de confusion $\\mathbf{M_c}$ qui résume de façon compacte les résultats de la classification:\n",
    "\n",
    "$$ \\mathbf{M_c} = \\left[ \\begin{matrix}\n",
    "TP & FP \\\\\n",
    "FN &  TN\n",
    "\\end{matrix} \\right]$$\n",
    "\n",
    "$TP$ : _True positive_, le nombre d'échantillons qui ont été labélisés comme contenant des chiens et qui contiennent effectivement des chiens. \n",
    "\n",
    "$FP$ : _False positive_, ou erreur de type I, le nombre d'échantillons qui ont été labelisés comme contenant des chiens alors qu'ils n'en contiennent pas.\n",
    "\n",
    "$FN$ : _False negative_, ou erreur de type II, le nombre d'échantillons qui ont été labelisés comme ne contenant pas de chiens alors qu'ils en contiennent.\n",
    "\n",
    "$TN$ : _True negative_, le nombre d'échantillons qui ont été labélisés comme ne contenant pas de chiens et qui n'en contiennent effectivement pas.\n",
    "\n",
    "Nous pouvons ensuite définir le nombre $P$ d'échantillons positifs et le nombre $N$ d'échantillons négatifs (en réalité, pas après prédiction). La métrique que nous avons utilisé tout au long du cours et qui est souvent utilisé est l'**accuracy**, celle-ci est définie par:\n",
    "\n",
    "$$ \\mathbf{accuracy} = \\frac{TP + TN}{P + N} = \\frac{TP + TN}{TP + TN + FP + FN} = \\frac{tr(\\mathbf{M_c})}{TOTAL}$$\n",
    "\n",
    "Concrètement, elle représente le ratio entre le nombre d'échantillons bien classés (positifs ou négatifs) et le nombre total d'échantillons. Elle ne donne cependant aucune information sur les échantillons qui sont mal classés. Elle est souvent mal utilisée car elle n'est pertinente que dans le cas où nous avons autant de d'échantillons positifs que négatifs à classer. Il faut aussi les prédictions et les erreurs de prédiction soient de même importance, ce qui est rarement le cas.\n",
    "\n",
    "On utilise également deux autres grandeurs, la _**precision**_ et le _**recall**_, celles-ci sont définies par:\n",
    "\n",
    "$$ \\mathbf{precision} = \\frac{TP}{TP + FP}$$ \n",
    "\n",
    "$$ \\mathbf{recall} = \\frac{TP}{TP + FN} = \\frac{TP}{P} = TPR $$\n",
    "\n",
    "$TPR$ est le _True Positive Rate_, on peut également définir le _False Positive Rate_, $FPR = 1 - TPR = \\frac{FP}{N}$.\n",
    "\n",
    "L'image suivante résume assez bien ce que nous venons de voir, les éléments bien classés sont sur la partie gauche de l'image. \n",
    "\n",
    "<img src=\"img/Precisionrecall.svg\" title=\"Source: https://en.wikipedia.org/wiki/Precision_and_recall\">\n",
    "\n",
    "\n",
    "Enfin, pour essayer de concilier _recall_ et _precision_, il est possible d'utiliser le _F-score_, il s'agit de la moyenne harmonique des deux :\n",
    "\n",
    "$$ \\textrm{F-score} = \\mathbf{F_1} = 2\\cdot \\frac{\\mathbf{precision} \\cdot \\mathbf{recall}}{\\mathbf{precision} + \\mathbf{recall}}$$\n",
    "\n",
    "On définit plus généralement la mesure $\\mathbf{F_\\beta}$ qui permet de donner plus de poids à l'un ou à l'autre suivant la valeur de $\\beta$ (pour $\\beta \\in \\mathbb{R}^+$) :\n",
    "$$\\mathbf{F_\\beta} = (1+\\beta^2)\\cdot \\frac{\\mathbf{precision} \\cdot \\mathbf{recall}}{\\beta^2 \\cdot \\mathbf{precision} + \\mathbf{recall}} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un des points les plus importants de ce notebook et qu'il faut retenir absolument est qu'il faut choisir un critère d'évaluation qui soit cohérent avec la tâche que l'on souhaite accomplir. Aucune de ces mesures n'est meilleure que les autres, cela dépend du contexte. \n",
    "\n",
    "Par exemple, changeons de cas d'étude et prenons le cas d'un algorithme de détection de tumeurs dans des images médicales (scanner, IRM ou autre), ce que nous souhaitons dans ce cas c'est minimiser le nombre de faux négatifs, $FN$, c'est-à-dire que nous voulons minimiser le nombre de cas où l'algorithme prédit l'absence de tumeur dans l'image alors qu'il y en a une, les conséquences de telles erreurs sont évidentes. Dans ce cas, on ne se focalisera donc pas sur le $FP$, le cas où l'algorithme prédit la présence d'une tumeur alors qu'il n'y en a pas, cela donnera plus de travail aux médecins mais minimisera le risque de non-détection.\n",
    "\n",
    "#### Un exemple\n",
    "\n",
    "Commençons par nous convaincre que l'_accuracy_ n'est pas une bonne mesure [1], nous considérons un jeu de données sur le cancer du sein, il contient des données sur 286 femmes atteintes d'un cancer, parmi elles, 201 n'ont pas eu de récidive et 85 ont eu une récidive. Nous voulons construire un classifieur binaire utilisant 9 _features_ pour prédire la présence ou l'absence de récidive. Imaginons que nous avons construit trois modèles M1, M2 et M3, le modèle M1 prédit l'absence de récidive dans tous les cas, le modèle M2 prédit la présence de récidive dans tous les cas, et le modèle M3 est un peu moins radical et prédit 23 récidives (10 sont correctes) et 263 non récidives (188 sont correctes). Regardons leur _accuracy_ : \n",
    "\n",
    "$$\\mathbf{accuracy(M_1)} = 201/286 \\approx 70 \\%$$\n",
    "$$\\mathbf{accuracy(M_2)} = 85/286 \\approx 30 \\%$$\n",
    "$$\\mathbf{accuracy(M_3)} =  \\frac{10+188}{286} \\approx 69.23 \\%$$\n",
    "\n",
    "En utilisant seulement l'_accuracy_, nous aurions tendance à dire que les modèle M1 et M3 sont assez performants. Pourtant, un coup d'oeil rapide aux matrices de confusion suffit à nous montrer qu'ils sont très différents:\n",
    "\n",
    "$$ \\mathbf{M_{C1}} = \\left[ \\begin{matrix}\n",
    "0 & 0 \\\\\n",
    "85 &  201\n",
    "\\end{matrix} \\right]\n",
    "$$\n",
    "\n",
    "$$ \\mathbf{M_{C2}} = \\left[ \\begin{matrix}\n",
    "85 & 201 \\\\\n",
    "0 & 0 \n",
    "\\end{matrix} \\right]\n",
    "$$\n",
    "\n",
    "$$ \\mathbf{M_{C3}} = \\left[ \\begin{matrix}\n",
    "10 & 13 \\\\\n",
    "75 &  188\n",
    "\\end{matrix} \\right]\n",
    "$$\n",
    "\n",
    "M3 est le seul à prédire à la fois de vrais positifs et de vrais négatifs. Regardons maintenant les autres grandeurs:\n",
    "\n",
    " * Precision :\n",
    " $$\\mathbf{precision(M1)} = \\frac{0}{0} = NaN $$ \n",
    " $$\\mathbf{precision(M2)} = \\frac{85}{286} \\approx 0.30 $$ \n",
    " $$\\mathbf{precision(M3)} = \\frac{10}{23} \\approx 0.43 $$ \n",
    "\n",
    " * Recall\n",
    " \n",
    " $$\\mathbf{recall(M1)} = \\frac{0}{0+85} = 0 $$\n",
    " $$\\mathbf{recall(M2)} = \\frac{85}{0+85} = 1$$\n",
    " $$\\mathbf{recall(M3)} = \\frac{10}{10+75} \\approx 0.12 $$\n",
    " \n",
    " * F-score\n",
    " \n",
    " $$ \\mathbf{F1(M1)} = 0 $$\n",
    " $$ \\mathbf{F1(M2)} \\approx 0.46 $$\n",
    " $$ \\mathbf{F1(M3)} \\approx 0.19 $$\n",
    " \n",
    "Dans notre exemple, nous voulions maximiser le _recall_, ce qui revient à minimiser $FN$.\n",
    "\n",
    "\n",
    "\n",
    "## AUC - Area Under the Curve\n",
    "\n",
    "\n",
    "Pour mesurer la performance d'un classifieur binaire, on peut tracer la courbe ROC (Receiver Operating Characteristic), celle-ci représente la variation de 'performance' du classifieur lorsque le seuil de décision varie. Concrètement, c'est la courbe qui relie les points dans le plan _FPR_ et _TPR_ (ou _recall_) lorsqu'on fait varier le seuil. \n",
    "\n",
    "<img src=\"img/Roccurves.png\" title=\"Source: https://en.wikipedia.org/wiki/Receiver_operating_characteristic\">\n",
    "\n",
    "La diagonale représente une classifieur qui tirerait au hasard sa prédiction avec une probabilité 0.5. Si la courbe est au dessus de la diagonale, le classifieur fait mieux qu'un tirage aléatoire, si elle en dessous il fait moins bien (dans ce cas il suffit d'inverser les prédicitions pour en faire un meilleur). Mais pour comparer plusieurs classifieurs entre-eux, comparer les courbes entre-elles n'est pas la méthode la plus précise comme on peut le voir sur la figure ci-dessus [3]. Il faut utiliser une grandeur quantitative, l'aire sous la courbe (AUC), dont la valeur varie entre 0.5 et 1.0 pour un classifieur performant.\n",
    "\n",
    "Il est possible d'utiliser des tests statistiques pour vérifier que les performances d'un classifieurs sont meilleures, en terme d'AUC. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La pratique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from math import inf\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considérons un exemple simple de classification binaire, nous avons deux nuages de points générés suivant des loi normales : $\\mathcal{N}(\\mu_1,cov_1)$ et $\\mathcal{N}(\\mu_2,cov_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "test = 400\n",
    "\n",
    "mu1 = [1,1]\n",
    "cov1 = [[4,3],[3,3]]\n",
    "X1 = np.random.multivariate_normal(mu1, cov1, N)\n",
    "\n",
    "#mu2 = [4,7]\n",
    "mu2 = [4,5]\n",
    "cov2 = [[1,0.5],[0.5,4]]\n",
    "X2 = np.random.multivariate_normal(mu2, cov2, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X1[:,0],X1[:,1],marker='.',linestyle='')\n",
    "plt.plot(X2[:,0],X2[:,1],marker='.',linestyle='',color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore(X):\n",
    "    return((X - np.mean(X, axis=0))/np.std(X, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation labels \n",
    "y1 = np.zeros(N)\n",
    "y2 = np.ones(N)\n",
    "\n",
    "# Concatenation\n",
    "X = np.concatenate((X1,X2))\n",
    "X = zscore(X)\n",
    "y = np.concatenate((y1,y2))\n",
    "\n",
    "s = np.arange(2*N)\n",
    "np.random.shuffle(s)\n",
    "X = X[s]\n",
    "y = y[s]\n",
    "\n",
    "X_test = X[-test:,:]\n",
    "X_train = X[:-test,:]\n",
    "y_test = y[-test:]\n",
    "y_train = y[:-test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train[:,0],X_train[:,1],marker='.',linestyle='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Sequential()\n",
    "nn.add(Dense(5, input_shape=(2,), kernel_initializer='uniform'))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(Dense(1, kernel_initializer='uniform'))\n",
    "nn.add(Activation('sigmoid'))\n",
    "#print(nn.summary())\n",
    "\n",
    "nn.compile(optimizer=RMSprop(lr=0.01), loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "history = nn.fit(X_train,y_train, epochs=15, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# récupère les prédictions du classifieur sur la base de test\n",
    "y_pred = nn.predict(X_test)\n",
    "res = nn.evaluate(X_test,y_test,batch_size=test)\n",
    "# return [loss, bin_accuracy] sur la base de test\n",
    "print(\"Test binary accuracy: {}%\".format(round(res[1]*100,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul de la matrice de confusion\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Question : Implémentez les fonctions ci-dessous qui calculent les 4 coefficients de la matrice de confusion étant donnés $y_{pred}$ le vecteur des classes prédites par le classifieur et $y_{test}$ les vrais labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_positive(y_pred, y_test, threshold):\n",
    "    TP = 0\n",
    "    i = 0\n",
    "    while i < len(y_pred):\n",
    "        if y_pred[i]>=threshold and y_test[i]==1:\n",
    "            TP += 1\n",
    "        i += 1\n",
    "    return TP\n",
    "\n",
    "def false_positive(y_pred, y_test, threshold):\n",
    "    FP = 0\n",
    "    i = 0\n",
    "    while i < len(y_pred):\n",
    "        if y_pred[i]>=threshold and y_test[i]==0:\n",
    "            FP += 1\n",
    "        i += 1\n",
    "    return FP\n",
    "\n",
    "def false_negative(y_pred, y_test, threshold):\n",
    "    FN = 0\n",
    "    i = 0\n",
    "    while i < len(y_pred):\n",
    "        if y_pred[i]<threshold and y_test[i]==1:\n",
    "            FN += 1\n",
    "        i += 1\n",
    "    return FN\n",
    "\n",
    "def true_negative(y_pred, y_test, threshold):\n",
    "    TN = 0\n",
    "    i = 0\n",
    "    while i < len(y_pred):\n",
    "        if y_pred[i]<threshold and y_test[i]==0:\n",
    "            TN += 1\n",
    "        i += 1\n",
    "    return TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConfusionMatrix(y_pred, y_test,threshold):\n",
    "    mat_conf = np.empty(4)\n",
    "    mat_conf[0] = true_positive(y_pred, y_test, threshold)\n",
    "    mat_conf[1] = false_positive(y_pred, y_test, threshold)\n",
    "    mat_conf[2] = false_negative(y_pred, y_test, threshold)\n",
    "    mat_conf[3] = true_negative(y_pred, y_test, threshold)\n",
    "    print(mat_conf[0],mat_conf[1])\n",
    "    print(mat_conf[2],mat_conf[3])\n",
    "    return mat_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "mat_conf = ConfusionMatrix(y_pred, y_test, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, Recall and F-Factor\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Question : Implémentez les fonctions $precision$, $recall$ et $f\\_measure$ pour qu'elles retournent respectivement la métrique correspondante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(TP,FP):\n",
    "    precision = TP/(TP+FP)\n",
    "    return precision\n",
    "\n",
    "def recall(TP,P):\n",
    "    recall = TP/P\n",
    "    return recall\n",
    "\n",
    "def f_measure(precision,recall):\n",
    "    f_measure = 2 / (1/precision + 1/recall)\n",
    "    return f_measure\n",
    "def measure(y_pred, y_test,threshold):\n",
    "    TP = true_positive(y_pred, y_test, threshold)\n",
    "    FP = false_positive(y_pred, y_test, threshold)\n",
    "    FN = false_negative(y_pred, y_test, threshold)\n",
    "    TN = true_negative(y_pred, y_test, threshold)\n",
    "    P = sum(y_test == 1)\n",
    "    N = sum(y_test == 0)\n",
    "    return TP,FP,FN,TN,P,N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[TP,FP,FN,TN,P,N] = measure(y_pred, y_test, threshold)\n",
    "prec = precision(TP,FP)\n",
    "rec = recall (TP,P)\n",
    "F_factor = f_measure(prec,rec)\n",
    "print(TP,FP,FN,TN,P,N)\n",
    "print(\"precision = \",prec)\n",
    "print(\"recall = \",rec)\n",
    "print(\"F_factor = \",F_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracé de la courbe ROC\n",
    "\n",
    "Le pseudo-code de l'algorithme pour extraire les coordonnées des points de la courbe ROC est présenté ci-dessous.\n",
    "<img src=\"img/algo_roc\" style=\"width: 450px;\">\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Question : écrire le code de cet algorithme dans la fonction ci-dessous pour retourner une matrice ($N_{test},2$), avec $N_{test}$ la taille de la liste retournée par le pseudo-code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateROCpoints(y_pred, y_test):\n",
    "    P = sum(y_test == 1)\n",
    "    N = sum(y_test == 0)\n",
    "    if P <= 0 or N <= 0:\n",
    "        raise ValueError('P and N should be positive')\n",
    "    y_pred = y_pred.flatten()\n",
    "    y_test = y_test.flatten()\n",
    "    index_sorted = np.flip(np.argsort(y_pred, kind='mergesort'),axis=0)\n",
    "    y_pred_sorted = y_pred[index_sorted]\n",
    "    y_test_sorted = y_test[index_sorted]\n",
    "    FP = 0\n",
    "    TP = 0\n",
    "    R = list()\n",
    "    f_prev = -inf\n",
    "    i = 0\n",
    "    while i < len(y_pred_sorted):\n",
    "        if y_pred_sorted[i] != f_prev:\n",
    "            R.append([FP/N,TP/P])\n",
    "            f_prev = y_pred_sorted[i]\n",
    "        if y_test_sorted[i] == 1:\n",
    "            TP += 1\n",
    "        else:\n",
    "            FP += 1\n",
    "        i += 1\n",
    "    R.append([FP/N,TP/P])\n",
    "    # L'algo du pseudo-code retourne une liste de liste, R, mais pour tracer la courbe il est plus\n",
    "    # facile d'utiliser une matrice R_mat.\n",
    "    R_mat = np.empty((len(R),len(R[0])))\n",
    "    for i in range(len(R)):\n",
    "        R_mat[i,:] = R[i]\n",
    "    return R_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_mat = generateROCpoints(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(R_mat[:,0], R_mat[:,1])\n",
    "plt.plot([0,1],[0,1])\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aire sous la courbe ROC\n",
    "\n",
    "<img src=\"img/algo_auc\" style=\"width: 450px\">\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Question : écrire le code de cet algorithme dans la fonction AreaUnderCurve($y_{pred}, y_{test}$) ci-dessous.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trap_area(X1,X2,Y1,Y2):\n",
    "    base = abs(X1-X2)\n",
    "    height_avg = (Y1+Y2)/2\n",
    "    area = base*height_avg\n",
    "    return area\n",
    "\n",
    "def AreaUnderCurve(y_pred, y_test):\n",
    "    P = sum(y_test == 1)\n",
    "    N = sum(y_test == 0)\n",
    "    if P <= 0 or N <= 0:\n",
    "        raise ValueError('P and N should be positive')\n",
    "    y_pred = y_pred.flatten()\n",
    "    y_test = y_test.flatten()\n",
    "    index_sorted = np.flip(np.argsort(y_pred, kind='mergesort'),axis=0)\n",
    "    y_pred_sorted = y_pred[index_sorted]\n",
    "    y_test_sorted = y_test[index_sorted]\n",
    "    FP = 0\n",
    "    TP = 0\n",
    "    FP_prev = 0\n",
    "    TP_prev = 0\n",
    "    A = 0\n",
    "    f_prev = -inf\n",
    "    i = 0\n",
    "    while i < len(y_pred_sorted):\n",
    "        if y_pred_sorted[i] != f_prev:\n",
    "            A += trap_area(FP,FP_prev,TP,TP_prev)\n",
    "            f_prev = y_pred_sorted[i]\n",
    "            FP_prev = FP\n",
    "            TP_prev = TP\n",
    "        if y_test_sorted[i] == 1:\n",
    "            TP += 1\n",
    "        else:\n",
    "            FP += 1\n",
    "        i += 1\n",
    "    # Sous la forme d'une matrice, plus facile à plotter, à mettre directement dans l'algo plus tard\n",
    "    A += trap_area(N,FP_prev,N,TP_prev)\n",
    "    A = A/(P*N)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Area = AreaUnderCurve(y_pred, y_test)\n",
    "Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## À vous de jouer !\n",
    "\n",
    "### Un exemple dans le domaine médical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Pour le jeu de données suivant sur le cancer du sein (label 0 = tumeur bégnine, 1 = tumeur maligne) implémenter différents algorithmes de classification binaire (SVM, arbre, réseau de neurones) et comparez leur performance pour les grandeurs introduites précédemment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chargement des données\n",
    "names = ['id', 'clumpThick', 'unifCellSize', 'unifCellShape', 'margAdh', 'SECS', 'bareNuclei', 'blandChrom', 'normalNucl','mistoses','class']\n",
    "dataframe = pd.read_csv('data/breast-cancer-wisconsin.data', names=names, na_filter='?')\n",
    "data = dataframe.values\n",
    "X = data[:,1:-1]\n",
    "y = data[:,-1]\n",
    "# Les labels dans le dataset sont 2 et 4 au lieu des traditionnels 0 et 1, on les remplace.\n",
    "y[y == 2] = 0\n",
    "y[y == 4] = 1\n",
    "\n",
    "size_test = 100 # doit être plus petit que la taille du dataset\n",
    "\n",
    "X_train = X[:-size_test,:]\n",
    "y_train = y[:-size_test]\n",
    "X_test = X[-size_test:,:]\n",
    "y_test = y[-size_test:]\n",
    "\n",
    "prec_list = np.zeros(3)\n",
    "rec_list = np.zeros(3)\n",
    "fScrore_list = np.zeros(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "1er indice : commencez par regarder la répartition des classes. Que peut-on en dire ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Il y a {} échantillons avec une tumeur bégnine ({}%).'.format(len(y[y==0]),100*len(y[y==0])/len(y)))\n",
    "print('Il y a {} échantillons avec une tumeur maligne ({}%).'.format(len(y[y==1]),100*len(y[y==1])/len(y)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Réponse : la répartition est inégale, l'_accuracy_ ne sera pas une bonne mesure de la performance du classifieur. Si le classifieur prédit uniquement la présence de tumeurs bégnines, il aura une _acccuracy_ de 65% ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avec une SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySVC = SVC(kernel='linear')\n",
    "\n",
    "mySVC.fit(X=X_train,y=y_train)\n",
    "\n",
    "print('Accuracy avec tous les features : {}%'.format(100*sum(mySVC.predict(X_test) == y_test) / len(y_test)))\n",
    "\n",
    "\n",
    "\n",
    "# Try to do PCA to represent in 2D (seulement pour la représentation, parce qu'on perd quand même de l'info...)\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "print(pca.explained_variance_ratio_)\n",
    "X_proj = pca.transform(X)\n",
    "\n",
    "X_proj_train = X_proj[:-size_test,:]\n",
    "X_proj_test = X_proj[-size_test:,:]\n",
    "\n",
    "mySVC2D = SVC(kernel='linear')\n",
    "\n",
    "mySVC2D.fit(X=X_proj_train,y=y_train)\n",
    "w = mySVC2D.coef_[0]\n",
    "w0 = mySVC2D.intercept_\n",
    "XX = np.arange(-4.,4.,0.1)\n",
    "YY = -(w[0]*XX+w0)/w[1]\n",
    "plt.plot(XX,YY,'g')\n",
    "plt.plot(X_proj_train[y_train == 0,0],X_proj_train[y_train == 0,1],marker='.',linestyle='',color='b')\n",
    "plt.plot(X_proj_train[y_train == 1,0],X_proj_train[y_train == 1,1],marker='x',linestyle='',color='r')\n",
    "plt.show()\n",
    "print('Accuracy avec les deux composantes principales : {}%'.format(100*sum(mySVC2D.predict(X_proj_test) == y_test) / len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Matrice de confusion, precision, recall, F-score et AUC\n",
    "\n",
    "y_pred = mySVC.predict(X_test)\n",
    "print(\"Matrice de confusion :\\n\")\n",
    "mat = ConfusionMatrix(y_pred=y_pred, y_test=y_test, threshold=0.5)\n",
    "print('\\n')\n",
    "[TP,FP,FN,TN,P,N] = measure(y_pred, y_test, 0.5)\n",
    "prec = precision(TP,FP)\n",
    "rec = recall (TP,P)\n",
    "F_factor = f_measure(prec,rec)\n",
    "\n",
    "print(\"precision = \",prec)\n",
    "print(\"recall = \",rec)\n",
    "print(\"F_factor = \",F_factor)\n",
    "\n",
    "prec_list[0] = prec\n",
    "rec_list[0] = rec\n",
    "fScrore_list[0] = F_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avec une forêt aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRFC = RandomForestClassifier(n_estimators=20)\n",
    "myRFC.fit(X_train,y_train)\n",
    "\n",
    "print('Accuracy sur les données de test : {}%'.format(100*sum(myRFC.predict(X_test) == y_test) / len(y_test)))\n",
    "y_pred = myRFC.predict(X_test)\n",
    "print(\"Matrice de confusion :\\n\")\n",
    "mat = ConfusionMatrix(y_pred=y_pred, y_test=y_test, threshold=0.5)\n",
    "print('\\n')\n",
    "[TP,FP,FN,TN,P,N] = measure(y_pred, y_test, 0.5)\n",
    "prec = precision(TP,FP)\n",
    "rec = recall (TP,P)\n",
    "F_factor = f_measure(prec,rec)\n",
    "\n",
    "print(\"precision = \",prec)\n",
    "print(\"recall = \",rec)\n",
    "print(\"F_factor = \",F_factor)\n",
    "\n",
    "prec_list[1] = prec\n",
    "rec_list[1] = rec\n",
    "fScrore_list[1] = F_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avec un réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Sequential()\n",
    "nn.add(Dense(5, input_shape=(9,), kernel_initializer='uniform'))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(Dense(15, kernel_initializer='uniform'))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(Dense(1, kernel_initializer='uniform'))\n",
    "nn.add(Activation('sigmoid'))\n",
    "#print(nn.summary())\n",
    "\n",
    "# Au lieu de mettre un nombre fixe d'époque, on en met un grand nombre (700 ici) et on définit le callback \n",
    "# earlystoppping pour s'arrêter après 15 epochs sans améliorétions de la 'binary_accuracy'\n",
    "earltstop = EarlyStopping(monitor='binary_accuracy', patience=5)\n",
    "\n",
    "nn.compile(optimizer=RMSprop(lr=0.0001), loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "history = nn.fit(X_train,y_train, epochs=700, batch_size=10, callbacks=[earltstop])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [x[0] for x in nn.predict(X_test, batch_size=10)]\n",
    "y_pred_lab = []\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] >= 0.5:\n",
    "        y_pred_lab.append(1)\n",
    "    else:\n",
    "        y_pred_lab.append(0)\n",
    "        \n",
    "print('Accuracy sur les données de test : {}%'.format(100*sum(y_pred_lab == y_test) / len(y_test)))\n",
    "        \n",
    "print(\"Matrice de confusion :\\n\")\n",
    "mat = ConfusionMatrix(y_pred=y_pred_lab, y_test=y_test, threshold=0.5)\n",
    "print('\\n')\n",
    "[TP,FP,FN,TN,P,N] = measure(y_pred_lab, y_test, 0.5)\n",
    "prec = precision(TP,FP)\n",
    "rec = recall (TP,P)\n",
    "F_factor = f_measure(prec,rec)\n",
    "\n",
    "print(\"precision = \",prec)\n",
    "print(\"recall = \",rec)\n",
    "print(\"F_factor = \",F_factor)\n",
    "\n",
    "prec_list[2] = prec\n",
    "rec_list[2] = rec\n",
    "fScrore_list[2] = F_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2)\n",
    "ax[0,0].bar([1,2,3],prec_list,color=['r','b','g'])\n",
    "ax[0,0].get_xaxis().set_visible(False)\n",
    "ax[0,0].set_title('Precision')\n",
    "ax[0,1].bar([1,2,3],rec_list,color=['r','b','g'])\n",
    "ax[0,1].get_xaxis().set_visible(False)\n",
    "ax[0,1].set_title('Recall')\n",
    "ax[1,0].bar([1,2,3],fScrore_list,color=['r','b','g'])\n",
    "ax[1,0].set_title('F-Score')\n",
    "ax[1,0].get_xaxis().set_visible(False)\n",
    "ax[1,1].get_xaxis().set_visible(False)\n",
    "ax[1,1].get_yaxis().set_visible(False)\n",
    "\n",
    "# En rouge la SVM, en bleu la forêt aléatoire et en vert le réseau de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Quelle conclusion tirez-vous de vos différentes expériences sur ce jeu de données? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Réponse : \n",
    " * Pour test_size=100, on constate que le réseau de neurones (dans la configuration ci-dessus, sujet à variation, mais les autres sont stables) a une _precision_ et un _F-Score_  moins élevé mais un _recall_ plus élevé que la forêt aléatoire qui a pourtant une _precision_ et un _F-Score_ plus élevé. Donc dans notre application où nous voulons minimiser _FN_, nous chosirions plutôt la SVM qui offre une bonne _precision_ mais surtout un bon _recall_. \n",
    " * Si les données étaient moins facilement discernables, la matrice de confusion aurait des termes plus important sur l'antidiagonale et on pourrait se poser des questions comme : Que se passe-t-il si vous faites varier la taille de l'ensemble de test ? Si vous faites varier les paramètres des classifieurs ? Dans notre cas, les variations sont minimes et les termes antidiagonaux très petits (0, 1 voire 2) et les effets ne sont pas vraiment visibles.\n",
    " * L'important ici est surtout d'avoir bien compris la démarche et l'intérêt des différentes grandeurs étudiées en particulier, lorsqu'on est confronté à un problème il est important de prendre un peu de recul et de se demander quelle est la métrique la plus adaptée à ce problème.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régression\n",
    "\n",
    "Le cas de la régression est moins problématique, pour mesurer la performance d'un modèle de régression, il suffit de mesurer son l'écart entre la prédiction et la vraie valeur (Mean Squared Error, Mean Absolute Error, qui donnent un ordre de grandeur de la magnitude de l'écart mais pas de signe). On peut également calculer le coefficient de détermination $R^2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## References\n",
    "\n",
    "[1] https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/\n",
    "\n",
    "[2] https://machinelearningmastery.com/assessing-comparing-classifier-performance-roc-curves-2/\n",
    "\n",
    "[3] https://en.wikipedia.org/wiki/Receiver_operating_characteristic\n",
    "\n",
    "[4] Les fichiers pdf dans l'archive\n",
    "\n",
    "[5] O. L. Mangasarian and W. H. Wolberg: \"Cancer diagnosis via linear \n",
    "      programming\", SIAM News, Volume 23, Number 5, September 1990, pp 1 & 18.\n",
    "\n",
    "<img src=\"img/img.jpg\" style=\"width: 450px;\" title=\"Machine learning memes for convolutional teens\">\n",
    "\n",
    "<img src=\"img/headache.png\" style=\"width: 450px;\" title=\"Machine learning memes for convolutional teens\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
