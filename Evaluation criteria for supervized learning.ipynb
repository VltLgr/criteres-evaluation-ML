{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critères d'évaluation en apprentissage supervisé\n",
    "\n",
    "## Théorie\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Tout au long de ce cours nous avons étudié différentes méthodes d'apprentissage supervisé, et à chaque fois nous avons cherché à optimiser l'_accuracy_, c'est-à-dire à minimiser l'erreur de généralisation. Mais cette approche pour évaluer la pertinence de notre modèle et sa robustesse est-elle suffisante ? En particulier dans ce notebook nous allons nous intéresser au cas de la classification binaire (deux classes), nous allons voir que dans le cas de la classification, plusieurs métriques interviennent, elles peuvent être contradictoires (dans le sens où elles varient de manière opposée) et il sera donc nécessaire de faire un compromis sur certaines grandeurs en fonction de l'application que l'on considère. \n",
    "\n",
    "Considérons un exemple de classification binaire que nous déroulerons tout au long de cette partie théorique pour illustrer nos propos. Nous allons considérer une tâche de classification simple consistant à identifier des chiens dans des images, notre système prend donc en entrée des images et prédit en sortie la présence ou l'absence de chien dans cette image.\n",
    "\n",
    "### Matrice de confusion\n",
    "\n",
    "Une métrique très souvent utilisée en classification est la matrice de confusion $\\mathbf{M_c}$ qui résume de façon compacte les résultats de la classification:\n",
    "\n",
    "$$ \\mathbf{M_c} = \\left[ \\begin{matrix}\n",
    "TP & FP \\\\\n",
    "FN &  TN\n",
    "\\end{matrix} \\right]$$\n",
    "\n",
    "$TP$ : _True positive_, le nombre d'échantillons qui ont été labélisés comme contenant des chiens et qui contiennent effectivement des chiens. \n",
    "\n",
    "$FP$ : _False positive_, ou erreur de type I, le nombre d'échantillons qui ont été labelisés comme contenant des chiens alors qu'ils n'en contiennent pas.\n",
    "\n",
    "$FN$ : _False negative_, ou erreur de type II, le nombre d'échantillons qui ont été labelisés comme ne contenant pas de chiens alors qu'ils en contiennent.\n",
    "\n",
    "$TN$ : _True negative_, le nombre d'échantillons qui ont été labélisés comme ne contenant pas de chiens et qui n'en contiennent effectivement pas.\n",
    "\n",
    "Nous pouvons ensuite définir le nombre $P$ d'échantillons positifs et le nombre $N$ d'échantillons négatifs (en réalité, pas après prédiction). La métrique que nous avons utilisé tout au long du cours et qui est souvent utilisé est l'**accuracy**, celle-ci est définie par:\n",
    "\n",
    "$$ \\mathbf{accuracy} = \\frac{TP + TN}{P + N} = \\frac{TP + TN}{TP + TN + FP + FN} = \\frac{tr(\\mathbf{M_c})}{TOTAL}$$\n",
    "\n",
    "Concrètement, elle représente le ratio entre le nombre d'échantillons bien classés (positifs ou négatifs) et le nombre total d'échantillons. Elle ne donne cependant aucune information sur les échantillons qui sont mal classés. Elle est souvent mal utilisée car elle n'est pertinente que dans le cas où nous avons autant de d'échantillons positifs que négatifs à classer. Il faut aussi les prédictions et les erreurs de prédiction soient de même importance, ce qui est rarement le cas.\n",
    "\n",
    "On utilise également deux autres grandeurs, la _**precision**_ et le _**recall**_, celles-ci sont définies par:\n",
    "\n",
    "$$ \\mathbf{precision} = \\frac{TP}{TP + FP}$$ \n",
    "\n",
    "$$ \\mathbf{recall} = \\frac{TP}{TP + FN} = \\frac{TP}{P} = TPR $$\n",
    "\n",
    "$TPR$ est le _True Positive Rate_, on peut également définir le _False Positive Rate_, $FPR = 1 - TPR = \\frac{FP}{N}$.\n",
    "\n",
    "L'image suivante résume assez bien ce que nous venons de voir, les éléments bien classés sont sur la partie gauche de l'image. \n",
    "\n",
    "<img src=\"img/Precisionrecall.svg\" title=\"Source: https://en.wikipedia.org/wiki/Precision_and_recall\">\n",
    "\n",
    "\n",
    "Enfin, pour essayer de concilier _recall_ et _precision_, il est possible d'utiliser le _F-score_, il s'agit de la moyenne harmonique des deux :\n",
    "\n",
    "$$ \\textrm{F-score} = \\mathbf{F_1} = 2\\cdot \\frac{\\mathbf{precision} \\cdot \\mathbf{recall}}{\\mathbf{precision} + \\mathbf{recall}}$$\n",
    "\n",
    "On définit plus généralement la mesure $\\mathbf{F_\\beta}$ qui permet de donner plus de poids à l'un ou à l'autre suivant la valeur de $\\beta$ (pour $\\beta \\in \\mathbb{R}^+$) :\n",
    "$$\\mathbf{F_\\beta} = (1+\\beta^2)\\cdot \\frac{\\mathbf{precision} \\cdot \\mathbf{recall}}{\\beta^2 \\cdot \\mathbf{precision} + \\mathbf{recall}} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un des points les plus importants de ce notebook et qu'il faut retenir absolument est qu'il faut choisir un critère d'évaluation qui soit cohérent avec la tâche que l'on souhaite accomplir. Aucune de ces mesures n'est meilleure que les autres, cela dépend du contexte. \n",
    "\n",
    "Par exemple, changeons de cas d'étude et prenons le cas d'un algorithme de détection de tumeurs dans des images médicales (scanner, IRM ou autre), ce que nous souhaitons dans ce cas c'est minimiser le nombre de faux négatifs, $FN$, c'est-à-dire que nous voulons minimiser le nombre de cas où l'algorithme prédit l'absence de tumeur dans l'image alors qu'il y en a une, les conséquences de telles erreurs sont évidentes. Dans ce cas, on ne se focalisera donc pas sur le $FP$, le cas où l'algorithme prédit la présence d'une tumeur alors qu'il n'y en a pas, cela donnera plus de travail aux médecins mais minimisera le risque de non-détection.\n",
    "\n",
    "#### Un exemple\n",
    "\n",
    "Commençons par nous convaincre que l'_accuracy_ n'est pas une bonne mesure [1], nous considérons un jeu de données sur le cancer du sein, il contient des données sur 286 femmes atteintes d'un cancer, parmi elles, 201 n'ont pas eu de récidive et 85 ont eu une récidive. Nous voulons construire un classifieur binaire utilisant 9 _features_ pour prédire la présence ou l'absence de récidive. Imaginons que nous avons construit trois modèles M1, M2 et M3, le modèle M1 prédit l'absence de récidive dans tous les cas, le modèle M2 prédit la présence de récidive dans tous les cas, et le modèle M3 est un peu moins radical et prédit 23 récidives (10 sont correctes) et 263 non récidives (188 sont correctes). Regardons leur _accuracy_ : \n",
    "\n",
    "$$\\mathbf{accuracy(M_1)} = 201/286 \\approx 70 \\%$$\n",
    "$$\\mathbf{accuracy(M_2)} = 85/286 \\approx 30 \\%$$\n",
    "$$\\mathbf{accuracy(M_3)} =  \\frac{10+188}{286} \\approx 69.23 \\%$$\n",
    "\n",
    "En utilisant seulement l'_accuracy_, nous aurions tendance à dire que les modèle M1 et M3 sont assez performants. Pourtant, un coup d'oeil rapide aux matrices de confusion suffit à nous montrer qu'ils sont très différents:\n",
    "\n",
    "$$ \\mathbf{M_{C1}} = \\left[ \\begin{matrix}\n",
    "0 & 0 \\\\\n",
    "85 &  201\n",
    "\\end{matrix} \\right]\n",
    "$$\n",
    "\n",
    "$$ \\mathbf{M_{C2}} = \\left[ \\begin{matrix}\n",
    "85 & 201 \\\\\n",
    "0 & 0 \n",
    "\\end{matrix} \\right]\n",
    "$$\n",
    "\n",
    "$$ \\mathbf{M_{C3}} = \\left[ \\begin{matrix}\n",
    "10 & 13 \\\\\n",
    "75 &  188\n",
    "\\end{matrix} \\right]\n",
    "$$\n",
    "\n",
    "M3 est le seul à prédire à la fois de vrais positifs et de vrais négatifs. Regardons maintenant les autres grandeurs:\n",
    "\n",
    " * Precision :\n",
    " $$\\mathbf{precision(M1)} = \\frac{0}{0} = NaN $$ \n",
    " $$\\mathbf{precision(M2)} = \\frac{85}{286} \\approx 0.30 $$ \n",
    " $$\\mathbf{precision(M3)} = \\frac{10}{23} \\approx 0.43 $$ \n",
    "\n",
    " * Recall\n",
    " \n",
    " $$\\mathbf{recall(M1)} = \\frac{0}{0+85} = 0 $$\n",
    " $$\\mathbf{recall(M2)} = \\frac{85}{0+85} = 1$$\n",
    " $$\\mathbf{recall(M3)} = \\frac{10}{10+75} \\approx 0.12 $$\n",
    " \n",
    " * F-score\n",
    " \n",
    " $$ \\mathbf{F1(M1)} = 0 $$\n",
    " $$ \\mathbf{F1(M2)} \\approx 0.46 $$\n",
    " $$ \\mathbf{F1(M3)} \\approx 0.19 $$\n",
    " \n",
    "Dans notre exemple, nous voulions maximiser le _recall_, ce qui revient à minimiser $FN$.\n",
    "\n",
    "\n",
    "\n",
    "## AUC - Area Under the Curve\n",
    "\n",
    "\n",
    "Pour mesurer la performance d'un classifieur binaire, on peut tracer la courbe ROC (Receiver Operating Characteristic), celle-ci représente la variation de 'performance' du classifieur lorsque le seuil de décision varie. Concrètement, c'est la courbe qui relie les points dans le plan _FPR_ et _TPR_ (ou _recall_) lorsqu'on fait varier le seuil. \n",
    "\n",
    "<img src=\"img/Roccurves.png\" title=\"Source: https://en.wikipedia.org/wiki/Receiver_operating_characteristic\">\n",
    "\n",
    "La diagonale représente une classifieur qui tirerait au hasard sa prédiction avec une probabilité 0.5. Si la courbe est au dessus de la diagonale, le classifieur fait mieux qu'un tirage aléatoire, si elle en dessous il fait moins bien (dans ce cas il suffit d'inverser les prédicitions pour en faire un meilleur). Mais pour comparer plusieurs classifieurs entre-eux, comparer les courbes entre-elles n'est pas la méthode la plus précise comme on peut le voir sur la figure ci-dessus [3]. Il faut utiliser une grandeur quantitative, l'aire sous la courbe (AUC), dont la valeur varie entre 0.5 et 1.0 pour un classifieur performant.\n",
    "\n",
    "Il est possible d'utiliser des tests statistiques pour vérifier que les performances d'un classifieurs sont meilleures, en terme d'AUC. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La pratique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from math import inf\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considérons un exemple simple de classification binaire, nous avons deux nuages de points générés suivant des loi normales : $\\mathcal{N}(\\mu_1,cov_1)$ et $\\mathcal{N}(\\mu_2,cov_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "test = 400\n",
    "\n",
    "mu1 = [1,1]\n",
    "cov1 = [[4,3],[3,3]]\n",
    "X1 = np.random.multivariate_normal(mu1, cov1, N)\n",
    "\n",
    "#mu2 = [4,7]\n",
    "mu2 = [4,5]\n",
    "cov2 = [[1,0.5],[0.5,4]]\n",
    "X2 = np.random.multivariate_normal(mu2, cov2, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X1[:,0],X1[:,1],marker='.',linestyle='')\n",
    "plt.plot(X2[:,0],X2[:,1],marker='.',linestyle='',color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore(X):\n",
    "    return((X - np.mean(X, axis=0))/np.std(X, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation labels \n",
    "y1 = np.zeros(N)\n",
    "y2 = np.ones(N)\n",
    "\n",
    "# Concatenation\n",
    "X = np.concatenate((X1,X2))\n",
    "X = zscore(X)\n",
    "y = np.concatenate((y1,y2))\n",
    "\n",
    "s = np.arange(2*N)\n",
    "np.random.shuffle(s)\n",
    "X = X[s]\n",
    "y = y[s]\n",
    "\n",
    "X_test = X[-test:,:]\n",
    "X_train = X[:-test,:]\n",
    "y_test = y[-test:]\n",
    "y_train = y[:-test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train[:,0],X_train[:,1],marker='.',linestyle='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Sequential()\n",
    "nn.add(Dense(5, input_shape=(2,), kernel_initializer='uniform'))\n",
    "nn.add(Activation('relu'))\n",
    "#nn.add(Dense(15, kernel_initializer='uniform'))\n",
    "#nn.add(Activation('relu'))\n",
    "nn.add(Dense(1, kernel_initializer='uniform'))\n",
    "nn.add(Activation('sigmoid'))\n",
    "#print(nn.summary())\n",
    "\n",
    "nn.compile(optimizer=RMSprop(lr=0.01), loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "history = nn.fit(X_train,y_train, epochs=15, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# récupère les prédictions du classifieur sur la base de test\n",
    "y_pred = nn.predict(X_test)\n",
    "res = nn.evaluate(X_test,y_test,batch_size=test)\n",
    "# return [loss, bin_accuracy] sur la base de test\n",
    "print(\"Test binary accuracy: {}%\".format(round(res[1]*100,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul de la matrice de confusion\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Question : Implémentez les fonctions ci-dessous qui calculent les 4 coefficients de la matrice de confusion étant donnés $y_{pred}$ le vecteur des classes prédites par le classifieur et $y_{test}$ les vrais labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_positive(y_pred, y_test, threshold):\n",
    "    return None\n",
    "\n",
    "def false_positive(y_pred, y_test, threshold):\n",
    "    return None\n",
    "\n",
    "def false_negative(y_pred, y_test, threshold):\n",
    "    return None\n",
    "\n",
    "def true_negative(y_pred, y_test, threshold):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConfusionMatrix(y_pred, y_test,threshold):\n",
    "    mat_conf = np.empty(4)\n",
    "    mat_conf[0] = true_positive(y_pred, y_test, threshold)\n",
    "    mat_conf[1] = false_positive(y_pred, y_test, threshold)\n",
    "    mat_conf[2] = false_negative(y_pred, y_test, threshold)\n",
    "    mat_conf[3] = true_negative(y_pred, y_test, threshold)\n",
    "    print(mat_conf[0],mat_conf[1])\n",
    "    print(mat_conf[2],mat_conf[3])\n",
    "    return mat_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "mat_conf = ConfusionMatrix(y_pred, y_test, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, Recall and F-Factor\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Question : Implémentez les fonctions $precision$, $recall$ et $f\\_measure$ pour qu'elles retournent respectivement la métrique correspondante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(TP,FP):\n",
    "    return None\n",
    "\n",
    "def recall(TP,P):\n",
    "    return None\n",
    "\n",
    "def f_measure(precision,recall):\n",
    "    return None\n",
    "\n",
    "def measure(y_pred, y_test,threshold):\n",
    "    TP = true_positive(y_pred, y_test, threshold)\n",
    "    FP = false_positive(y_pred, y_test, threshold)\n",
    "    FN = false_negative(y_pred, y_test, threshold)\n",
    "    TN = true_negative(y_pred, y_test, threshold)\n",
    "    P = sum(y_test == 1)\n",
    "    N = sum(y_test == 0)\n",
    "    return TP,FP,FN,TN,P,N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[TP,FP,FN,TN,P,N] = measure(y_pred, y_test, threshold)\n",
    "prec = precision(TP,FP)\n",
    "rec = recall (TP,P)\n",
    "F_factor = f_measure(prec,rec)\n",
    "print(TP,FP,FN,TN,P,N)\n",
    "print(\"precision = \",prec)\n",
    "print(\"recall = \",rec)\n",
    "print(\"F_factor = \",F_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracé de la courbe ROC\n",
    "\n",
    "Le pseudo-code de l'algorithme pour extraire les coordonnées des points de la courbe ROC est présenté ci-dessous.\n",
    "<img src=\"img/algo_roc\" style=\"width: 450px;\">\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Question : écrire le code de cet algorithme dans la fonction ci-dessous pour retourner une matrice ($N_{test},2$), avec $N_{test}$ la taille de la liste retournée par le pseudo-code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateROCpoints(y_pred, y_test):\n",
    "    ## Mettre le code ici ###\n",
    "    R = [[None]] ### A enlever (juste pour éviter quelques erreurs tant que le code n'est pas rempli)\n",
    "    \n",
    "    \n",
    "    ########################\n",
    "    # A ce point de l'algo vous devriez avoir R sous la forme R = [[x0,y0],[x1,y1],...,[xN,yN]]\n",
    "    # L'algo du pseudo-code retourne une liste de liste, R, mais pour tracer la courbe il est plus\n",
    "    # facile d'utiliser une matrice R_mat.\n",
    "    R_mat = np.empty((len(R),len(R[0])))\n",
    "    for i in range(len(R)):\n",
    "        R_mat[i,:] = R[i]\n",
    "    return R_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_mat = generateROCpoints(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decommenter la ligne suivante une fois la fonction generateROCpoints implémentée correctement\n",
    "#plt.plot(R_mat[:,0], R_mat[:,1])    #### A DECOMMENTER \n",
    "plt.plot([0,1],[0,1])\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aire sous la courbe ROC\n",
    "\n",
    "<img src=\"img/algo_auc\" style=\"width: 450px\">\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Question : écrire le code de cet algorithme dans la fonction AreaUnderCurve($y_{pred}, y_{test}$) ci-dessous.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trap_area(X1,X2,Y1,Y2):\n",
    "    base = abs(X1-X2)\n",
    "    height_avg = (Y1+Y2)/2\n",
    "    area = base*height_avg\n",
    "    return area\n",
    "\n",
    "def AreaUnderCurve(y_pred, y_test):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Area = AreaUnderCurve(y_pred, y_test)\n",
    "Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## À vous de jouer !\n",
    "\n",
    "### Un exemple dans le domaine médical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Pour le jeu de données suivant sur le cancer du sein (label 0 = tumeur bégnine, 1 = tumeur maligne) implémenter différents algorithmes de classification binaire (SVM, arbre, réseau de neurones) et comparez leur performance pour les grandeurs introduites précédemment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chargement des données\n",
    "names = ['id', 'clumpThick', 'unifCellSize', 'unifCellShape', 'margAdh', 'SECS', 'bareNuclei', 'blandChrom', 'normalNucl','mistoses','class']\n",
    "dataframe = pd.read_csv('data/breast-cancer-wisconsin.data', names=names, na_filter='?')\n",
    "data = dataframe.values\n",
    "X = data[:,1:-1]\n",
    "y = data[:,-1]\n",
    "# Les labels dans le dataset sont 2 et 4 au lieu des traditionnels 0 et 1, on les remplace.\n",
    "y[y == 2] = 0\n",
    "y[y == 4] = 1\n",
    "\n",
    "size_test = 200 # doit être plus petit que la taille du dataset\n",
    "\n",
    "X_train = X[:-size_test,:]\n",
    "y_train = y[:-size_test]\n",
    "X_test = X[-size_test:,:]\n",
    "y_test = y[-size_test:]\n",
    "\n",
    "# Pour plotter un plt.bar afin de comparer les trois classifieurs\n",
    "prec_list = np.zeros(3)\n",
    "rec_list = np.zeros(3)\n",
    "fScrore_list = np.zeros(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "1er indice : commencez par regarder la répartition des classes. Que peut-on en dire ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Réponse : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avec une SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avec une forêt aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avec un réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2)\n",
    "ax[0,0].bar([1,2,3],prec_list,color=['r','b','g'])\n",
    "ax[0,0].get_xaxis().set_visible(False)\n",
    "ax[0,0].set_title('Precision')\n",
    "ax[0,1].bar([1,2,3],rec_list,color=['r','b','g'])\n",
    "ax[0,1].get_xaxis().set_visible(False)\n",
    "ax[0,1].set_title('Recall')\n",
    "ax[1,0].bar([1,2,3],fScrore_list,color=['r','b','g'])\n",
    "ax[1,0].set_title('F-Score')\n",
    "ax[1,0].get_xaxis().set_visible(False)\n",
    "ax[1,1].get_xaxis().set_visible(False)\n",
    "ax[1,1].get_yaxis().set_visible(False)\n",
    "\n",
    "# En rouge la SVM, en bleu la forêt aléatoire et en vert le réseau de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Quelle conclusion tirez-vous de vos différentes expériences sur ce jeu de données? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Réponse : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régression\n",
    "\n",
    "Le cas de la régression est moins problématique, pour mesurer la performance d'un modèle de régression, il suffit de mesurer son l'écart entre la prédiction et la vraie valeur (Mean Squared Error, Mean Absolute Error, qui donnent un ordre de grandeur de la magnitude de l'écart mais pas de signe). On peut également calculer le coefficient de détermination $R^2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## References\n",
    "\n",
    "[1] https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/\n",
    "\n",
    "[2] https://machinelearningmastery.com/assessing-comparing-classifier-performance-roc-curves-2/\n",
    "\n",
    "[3] https://en.wikipedia.org/wiki/Receiver_operating_characteristic\n",
    "\n",
    "[4] Les fichiers pdf dans l'archive\n",
    "\n",
    "[5] O. L. Mangasarian and W. H. Wolberg: \"Cancer diagnosis via linear \n",
    "      programming\", SIAM News, Volume 23, Number 5, September 1990, pp 1 & 18.\n",
    "\n",
    "<img src=\"img/img.jpg\" style=\"width: 450px;\" title=\"Machine learning memes for convolutional teens\">\n",
    "\n",
    "<img src=\"img/headache.png\" style=\"width: 450px;\" title=\"Machine learning memes for convolutional teens\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
